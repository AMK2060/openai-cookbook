{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Voice Chatbot with GPT-4o Audio Modality \n",
    "\n",
    "In this cookbook, we will walk through the process of building a voice enabled sales chatbot with newly released GPT-4o audio-in and audio-out modalities. In addition, to the text and image modalities, GPT-4o enables you to generate a spoken audio response to a prompt, and to use audio inputs to prompt the model. You can learn more about Audio Generation capabilities of GPT-4o [here](https://platform.openai.com/docs/guides/audio).  \n",
    "\n",
    "GPT-4o audio modality based chatbot provide a balance between low-latency in communication and better control over the conversation. With streaming audio output, GPT-4o is an improvement over the previous generation of Text-To-Speech (TTS)/Speech-To-Text (STT) chatbots.  \n"
   ],
   "id": "f5f09c39d2c92df9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Creating an GPT-4o voice modality based chatbot is a three-step process, as outlined below:\n",
    "\n",
    "**1. Set Up the GPT Model with Audio-Out Modality**  \n",
    "Initialize the GPT model with system prompts that define the goal of the conversation, guiding the chatbot's responses toward assisting with sales order placement. The prompts can be set up for a multi-assistant system, where one assistant drives the conversation with the customer and another assistant manages the cart in parallel. Also, set up tools for assistants to use when asking for human help or interacting with each other (such as cart pricing).\n",
    "\n",
    "**2. Develop Audio Modules for ASR (Automatic Speech Recognition) and Setup GPT-4o for Audio-input**  \n",
    " Create an audio interface that listens to the user, and records their speech. Implement a **VAD (Voice Activity Detection) module** for a **handsfree operation**. This module detects input audio from user and segments the audio at silence intervals to send to Whisper model for transcription. Keep VAD (Voice Activity Detection) module parameters (threshold of audio amplitude that qualifies as silence, and the duration of silent chunks) configurable, so they can be adjusted based on the environment. The audio from the user is sent to the GPT-4o model. \n",
    " \n",
    "\n",
    "**3. Create a conversation loop and manage order cart**  \n",
    "Implement a conversation loop where the agent listens to the user and responds back, continuing until an event occurs that breaks the loop, such as a request to speak with a human or another indication of the end of the conversation."
   ],
   "id": "5b0abac4f4103239"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the purposes of this cookbook, we will use an example of an office stationery ordering bot. You can interact with the bot to order general-purpose office products such as pencils, pens, paper clips, writing pads, printing paper, and envelopes.\n",
    "\n",
    "The key challenges we want to address are:\n",
    "\n",
    "1. Ensure customers can only order items that are available.\n",
    "2. Escalate to a human in the loop if the customer requests help or engages in non-order-related conversation.\n",
    "3. Provide an accurate summary of the order with prices to the customer.\n",
    "4. Minimize the lag in the conversation \n",
    "\n",
    "\n",
    "Before we get started, make sure you have the following libraries installed: `pyaudio`, `numpy`, `openai`, `playsound`, and that you have configured your OpenAI API key as an environment variable."
   ],
   "id": "ed54a56d131d5831"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Set Up the GPT Model \n",
    "\n",
    "First step is to set the foundation for the GPT model to operate effectively as a sales chatbot within the office stationery domain. By carefully crafting the prompts and defining the functions, we ensure that the bot can handle customer interactions smoothly, maintain the flow of conversation, and provide accurate assistance aligned with the objectives of our project.\n",
    "\n",
    "We will initiate a Sales Bot prompt `SALES_BOT_PROMPT` that would drive the interaction with the user, and a `SALES_CART_PROMPT` prompt that would manage the cart. Note that the list of items available for sale are provide as a list of JSON objects `office_stationery_items`. This helps the sales bot and sales cart assistant to understand the available items, and repond the user accordingly.  \n"
   ],
   "id": "aad28923ef73ba72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:50:56.607021Z",
     "start_time": "2024-10-23T22:50:56.601982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creates a list of dictionaries, where each dictionary represents an office stationery item available for purchase.\n",
    "office_stationery_items = [\n",
    "    {\"item-id\": \"0001\", \"item-name\": \"pencil\", \"item-price\": \"$0.50\"},\n",
    "    {\"item-id\": \"0002\", \"item-name\": \"pen\", \"item-price\": \"$1.00\"},\n",
    "    {\"item-id\": \"0003\", \"item-name\": \"clip\", \"item-price\": \"$0.05\"},\n",
    "    {\"item-id\": \"0004\", \"item-name\": \"writing pad\", \"item-price\": \"$2.00\"},\n",
    "    {\"item-id\": \"0005\", \"item-name\": \"printing paper\", \"item-price\": \"$5.00\"},\n",
    "    {\"item-id\": \"0006\", \"item-name\": \"envelope\", \"item-price\": \"$0.10\"}\n",
    "]\n",
    "\n",
    "# Defines the system prompt that instructs the GPT model on how to behave during the conversation.\n",
    "SALES_BOT_PROMPT = f\"\"\"You are a office stationery sales bot. The customer will ask to buy one of the following items. Follow the rules below: \n",
    "1. Be succinct in your responses up to 10 words or less if possible.  \n",
    "2. If the customer asks for an item that is not available, you should let the customer know that item is not available.\n",
    "3. Once the customer has placed an order, reply with ANYTHING ELSE\n",
    "4. If the customer wants to chat with a human, call the function  'get_human_help'\n",
    "5. If the customer discusses any other topic, other than ordering office stationery, call the function 'get_human_help'\n",
    "6. When the order is final, call the function `get_order_details` and let the customer know the price.\n",
    "<LIST OF ITEMS>\n",
    "{office_stationery_items}\n",
    "</LIST OF ITEMS>  \n",
    "\"\"\"\n",
    "\n",
    "# Provides a separate prompt to guide the bot in generating the final order cart\n",
    "# An example is provided to illustrate the desired output format, ensuring consistency and accuracy in the bot's response\n",
    "# This could be further enhanced by structured output, but one shot example is sufficient in this context \n",
    "SALES_CART_PROMPT = f\"\"\"You are an office stationery sales bot, that will generate a cart based on a conversation between a user and an agent. The list of items available for purchase is provided below. Output the cart in JSON format. Include quantity and total price of the order. \n",
    "\n",
    "<LIST OF ITEMS>\n",
    "{office_stationery_items}\n",
    "</LIST OF ITEMS> \n",
    "\n",
    "<EXAMPLE OF A CART> \n",
    "{{\n",
    "  \"cart\": [\n",
    "    {{\n",
    "      \"item-id\": \"0001\",\n",
    "      \"item-name\": \"pencil\",\n",
    "      \"quantity\": 4,\n",
    "      \"item-price\": \"$0.50\",\n",
    "      \"total-item-price\": \"$2.00\"\n",
    "    }}\n",
    "  ],\n",
    "  \"total-price\": \"$2.00\"\n",
    "}}\n",
    "</EXAMPLE OF A CART> \n",
    "\"\"\"\n",
    "\n",
    "# Defines functions that the bot can \"call\" during the conversation to handle specific situations such as to get order details and get human help \n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_order_details\",\n",
    "            \"description\": \"Use this function once the customer has finished ordering to get the order price.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_human_help\",\n",
    "            \"description\": \"Use this function if customer discusses topics other than the order or wants to speak with a human.\"\n",
    "        }\n",
    "    }]\n",
    "\n",
    "# Initialize the prompt for sales agent \n",
    "sales_agent_prompt = [{\"role\": \"system\", \"content\": SALES_BOT_PROMPT}]\n",
    "\n",
    "# Initialize the prompt for pricing agent \n",
    "pricing_agent_prompt = [{\"role\": \"system\", \"content\": SALES_CART_PROMPT}]"
   ],
   "id": "9d284c2c0322cc9d",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Develop Audio Modules for ASR (Automatic Speech Recognition) and TTS (text-to-speech)\n",
    "\n",
    "The following Python code implements an interactive voice agent that facilitates customer interactions for ordering office stationery.  \n",
    " \n",
    "The `listen()` function implements the **VAD (Voice Activity Detection) module** using `PyAudio` that streams the audio in `frames_per_buffer` defined as `CHUNK`. Each `CHUNK` is `1024` frames in the audio buffer. Function `is_silent(input_data)` determines if the audio data is below the `SILENCE_THRESHOLD` to classify the audio chunk as silent. This can help filter out low noise in the environment such as breathing sounds. If there are consecutive `50` `SILENT_CHUNKS`  as defined in the code below, the function interprets it as the customer has finished speaking, and saves the audio to a WAV file. To qualify as valid user input, the user must have spoken something which is determined using `SPOKEN_CHUNKS`. Once the **VAD (Voice Activity Detection) module** determines the user input is valid, and user has finished speaking, the audio is sent to OpenAI's Whisper model for **ASR (Automatic Speech Recognition)**, and transcription in English returned as part of the function call.  \n",
    "\n",
    "Variables `SPOKEN_CHUNKS`, `SILENCE_THRESHOLD` and `SILENT_CHUNKS` can be adjusted based on the environment and type of use case to segment the input audio. \n",
    "\n",
    "The `speak(agent_message)` function takes the agent's text response, converts it into spoken audio using OpenAI's text-to-speech model, saves it as a WAV file, and plays it back to the customer. Overall, the code enables a conversational interface by integrating speech recognition and synthesis. [Note that in this implementation audio cannot be interrupted once the function starts speaking. The user must wait for its turn to speak.] \n",
    "\n",
    "To reduce the lag, we have pre-recorded sound snippets and stored them under `sounds` folder. If agent response is one of these pre-recorded phrases we can play them instantaneously, reducing the perceived lag. "
   ],
   "id": "d2d984cfd07e7b16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:50:57.211267Z",
     "start_time": "2024-10-23T22:50:57.203444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024  # Number of frames per buffer\n",
    "FORMAT = pyaudio.paInt16  # Sample format\n",
    "CHANNELS = 1  # Mono audio\n",
    "RATE = 24000  # Sample rate (Hz)\n",
    "SILENCE_THRESHOLD = 20  # Silence threshold\n",
    "SILENT_CHUNKS = 50  # Number of silent chunks to stop recording\n",
    "SPOKEN_CHUNKS = 50  # Minimum spoken chunks to consider valid speech\n",
    "\n",
    "\n",
    "def listen():\n",
    "    \"\"\"Listen to the microphone and return the audio as a base64-encoded WAV string.\"\"\"\n",
    "    print(\"Agent listening ...\")\n",
    "\n",
    "    def is_silent(data_chunk):\n",
    "        \"\"\"Determine if the given audio chunk is silent.\"\"\"\n",
    "        audio_data = np.frombuffer(data_chunk, dtype=np.int16)\n",
    "        return np.abs(audio_data).mean() < SILENCE_THRESHOLD\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    frames = []\n",
    "    silent_chunks = speech_chunks = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "\n",
    "            if is_silent(data):\n",
    "                silent_chunks += 1\n",
    "            else:\n",
    "                silent_chunks = 0\n",
    "                speech_chunks += 1\n",
    "\n",
    "            if silent_chunks > SILENT_CHUNKS and speech_chunks > SPOKEN_CHUNKS:\n",
    "                break\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "    print(\"* done listening\")\n",
    "\n",
    "    # Write to an in-memory WAV file\n",
    "    audio_buffer = io.BytesIO()\n",
    "    with wave.open(audio_buffer, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    # Get the WAV data from the buffer\n",
    "    audio_data = audio_buffer.getvalue()\n",
    "\n",
    "    # Encode the WAV data to a base64 string\n",
    "    base64_encoded_audio = base64.b64encode(audio_data).decode('utf-8')\n",
    "\n",
    "    return base64_encoded_audio\n"
   ],
   "id": "c5e6d047c6c17830",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:56:34.086672Z",
     "start_time": "2024-10-23T22:56:34.084281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the API key from the environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def process_audio_with_gpt_4o(output_modalities, prompt_messages_dictionary, tools):\n",
    "    # Chat Completions API end point \n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "    # Set the headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Construct the request data\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-audio-preview\",\n",
    "        \"modalities\": output_modalities,\n",
    "        \"tools\":tools,\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"audio\": {\n",
    "            \"voice\": \"alloy\",\n",
    "            \"format\": \"wav\"\n",
    "        },\n",
    "        \"messages\": prompt_messages_dictionary\n",
    "    }\n",
    "    \n",
    "    request_response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if request_response.status_code == 200:\n",
    "        return request_response.json()\n",
    "    else:\n",
    "        print(f\"Error {request_response.status_code}: {request_response.text}\")\n",
    "        return"
   ],
   "id": "f6dd65a2ce0a6317",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "af1ae3abd1de1451"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:56:55.638974Z",
     "start_time": "2024-10-23T22:56:34.947242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make sure pydub is installed \n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from io import BytesIO\n",
    "\n",
    "messages_dictionary = []  # Store the messages \n",
    "\n",
    "# Loop until the user has completed the order or asks for human help \n",
    "while True:\n",
    "    # listen to the user input \n",
    "    user_input_base64_wav_audio = listen()\n",
    "\n",
    "    # Append the message to messages dictionary to pass on the model \n",
    "    messages_dictionary.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"input_audio\",\n",
    "                \"input_audio\": {\n",
    "                    \"data\": user_input_base64_wav_audio,\n",
    "                    \"format\": \"wav\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    messages = sales_agent_prompt + messages_dictionary\n",
    "    \n",
    "    \n",
    "\n",
    "    response_json = process_audio_with_gpt_4o([\"text\", \"audio\"], messages, TOOLS)\n",
    "    \n",
    "    print(response_json)\n",
    "    \n",
    "    tool_calls = \"\"\n",
    "        #response_json['choices'][0]['message'].get('tool_calls', None)\n",
    "    \n",
    "    if tool_calls:\n",
    "        print(\"tool call!\")\n",
    "        tool_function_name = tool_calls[0]['function']['name']\n",
    "        if tool_function_name == \"get_order_details\":\n",
    "            # The pricing agent generates a detailed cart in JSON format, including item quantities and total prices, ensuring the user receives an accurate summary of their order.\n",
    "            print(\"get order details\")\n",
    "            break;\n",
    "        elif tool_function_name == \"get_human_help\":\n",
    "            #  get_human_help function allows the assistant to gracefully transfer the conversation to a human agent if the user requests assistance or deviates from the order process.\n",
    "            print(\"Get human help\")\n",
    "            break;\n",
    "        else: \n",
    "            print(f\"Tool does not exist: {tool_function_name}\")\n",
    "            \n",
    "    else: \n",
    "        print(\"continue conversation!\")\n",
    "        response_message = response_json['choices'][0]['message']\n",
    "        \n",
    "        # Get the transcript from the model. This will vary depending on the modality you are using. \n",
    "        message_transcript = response_message['audio']['transcript']\n",
    "        \n",
    "        ## print(message_transcript)\n",
    "        \n",
    "        # Get the audio content from the response \n",
    "        message_audio = response_message['audio']['data']\n",
    "        \n",
    "        # Play the audio \n",
    "        audio_data_bytes = base64.b64decode(message_audio)\n",
    "        audio_segment = AudioSegment.from_file(BytesIO(audio_data_bytes), format=\"wav\")\n",
    "        \n",
    "        play(audio_segment)\n",
    "        \n",
    "        break\n",
    "\n",
    "    \n",
    "\n"
   ],
   "id": "ebe2e5f826cb6b64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent listening ...\n",
      "* done listening\n",
      "Error 500: {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_066b051d6398e2d8d8a1e22e468671d5 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      "None\n",
      "continue conversation!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 54\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m: \n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontinue conversation!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 54\u001B[0m     response_message \u001B[38;5;241m=\u001B[39m \u001B[43mresponse_json\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchoices\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# Get the transcript from the model. This will vary depending on the modality you are using. \u001B[39;00m\n\u001B[1;32m     57\u001B[0m     message_transcript \u001B[38;5;241m=\u001B[39m response_message[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranscript\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T22:55:28.556369Z",
     "start_time": "2024-10-23T22:55:28.554117Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b2f908aba972613f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf8e0cc6710990a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
